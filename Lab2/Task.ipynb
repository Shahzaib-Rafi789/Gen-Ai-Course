{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Text Collection and Loading\n",
    "Tweets Dataset: https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1     is upset that he can't update his Facebook by ...\n",
      "2     @Kenichan I dived many times for the ball. Man...\n",
      "3       my whole body feels itchy and like its on fire \n",
      "4     @nationwideclass no, it's not behaving at all....\n",
      "5                         @Kwesidei not the whole crew \n",
      "6                                           Need a hug \n",
      "7     @LOLTrish hey  long time no see! Yes.. Rains a...\n",
      "8                  @Tatiana_K nope they didn't have it \n",
      "9                             @twittera que me muera ? \n",
      "10          spring break in plain city... it's snowing \n",
      "11                           I just re-pierced my ears \n",
      "12    @caregiving I couldn't bear to watch it.  And ...\n",
      "13    @octolinz16 It it counts, idk why I did either...\n",
      "14    @smarrison i would've been the first, but i di...\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "dataset_path = \"Dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df[\"Tweet\"].head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text into sentences and words.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of words in the text.\n",
    "    \"\"\"\n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Word tokenization\n",
    "    words = word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"\n",
    "    Removes stop words from a list of words.\n",
    "    \n",
    "    Input:\n",
    "    - words (list): List of words from which stop words should be removed.\n",
    "    \n",
    "    Output:\n",
    "    - filtered_words (list): List of words with stop words removed.\n",
    "    \"\"\"\n",
    "    # Get the set of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words from the list\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem(words):\n",
    "    \"\"\"\n",
    "    Applies stemming to a list of words, reducing them to their root forms.\n",
    "\n",
    "    Args:\n",
    "    words (list): A list of words to be stemmed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of stemmed words.\n",
    "    \"\"\"\n",
    "    # Initialize the Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    # Apply stemming to each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_pos_tag):\n",
    "    if nltk_pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize(words):\n",
    "    \"\"\"\n",
    "    Applies lemmatization to a list of words, reducing them to their base forms.\n",
    "\n",
    "    Args:\n",
    "    words (list): A list of words to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lemmatized words.\n",
    "    \"\"\"\n",
    "    # Initialize the WordNet lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Get POS tags for the words\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    # Apply lemmatization to each word with its POS tag\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    \n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     @ switchfoot http : //twitpic.com/2y1zl - Awww...\n",
      "1     upset ca n't update Facebook texting ... might...\n",
      "2     @ Kenichan dive many time ball . Managed save ...\n",
      "3                       whole body feel itchy like fire\n",
      "4     @ nationwideclass , 's behaving . 'm mad . ? c...\n",
      "                            ...                        \n",
      "95    Strider sick little puppy http : //apps.facebo...\n",
      "96    rylee , grace ... wana go steve 's party ? ? S...\n",
      "97    hey , actually one bracket pool ! bad n't one ...\n",
      "98                   @ stark n't follow , either work !\n",
      "99    bad nite favorite team : Astros Spartans lose ...\n",
      "Name: Tweet, Length: 100, dtype: object\n",
      "['behaving']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Applies a series of preprocessing steps to a tweet.\n",
    "    \n",
    "    Input:\n",
    "    - tweet (str): The tweet text to be preprocessed.\n",
    "    \n",
    "    Output:\n",
    "    - preprocessed (str): Preprocessed text.\n",
    "    \"\"\"\n",
    "    # Tokenization\n",
    "    words = tokenize(tweet)\n",
    "    \n",
    "    # Stop Word Removal\n",
    "    words_no_stopwords = remove_stopwords(words)\n",
    "    \n",
    "    # Stemming    \n",
    "    stemmed_words = stem(words_no_stopwords)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = lemmatize(words_no_stopwords)\n",
    "    \n",
    "    # Return after all preprocessing\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply preprocessing to the 'Tweet' column\n",
    "# df['Preprocessed'] = df['Tweet'][:100].apply(preprocess_tweet)\n",
    "print(df['Tweet'][:100].apply(preprocess_tweet))\n",
    "print(lemmatize(['behaving']))\n",
    "\n",
    "# print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
