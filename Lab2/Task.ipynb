{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Text Collection and Loading\n",
    "Tweets Dataset: https://www.kaggle.com/datasets/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1     is upset that he can't update his Facebook by ...\n",
      "2     @Kenichan I dived many times for the ball. Man...\n",
      "3       my whole body feels itchy and like its on fire \n",
      "4     @nationwideclass no, it's not behaving at all....\n",
      "5                         @Kwesidei not the whole crew \n",
      "6                                           Need a hug \n",
      "7     @LOLTrish hey  long time no see! Yes.. Rains a...\n",
      "8                  @Tatiana_K nope they didn't have it \n",
      "9                             @twittera que me muera ? \n",
      "10          spring break in plain city... it's snowing \n",
      "11                           I just re-pierced my ears \n",
      "12    @caregiving I couldn't bear to watch it.  And ...\n",
      "13    @octolinz16 It it counts, idk why I did either...\n",
      "14    @smarrison i would've been the first, but i di...\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "dataset_path = \"Dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df[\"Tweet\"].head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "Tokenization is the process of splitting text into individual units called tokens. These tokens can be words, phrases, or sentences. In natural language processing (NLP), it typically involves dividing the text into words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text into sentences and words.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of words in the text.\n",
    "    \"\"\"\n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Word tokenization\n",
    "    words = word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization helps in breaking down the text into manageable pieces, making it easier to analyze and process further. It transforms a large body of text into smaller, more understandable units, which can be counted, tagged, or classified. Proper tokenization is crucial as it sets the stage for the following preprocessing steps. Poor tokenization can lead to inaccurate analysis and reduced performance of NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stops Words Removal\n",
    "Stop word removal involves eliminating common words that are often considered irrelevant for text analysis. Examples of stop words include \"and\", \"the\", \"is\", and \"in\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"\n",
    "    Removes stop words from a list of words.\n",
    "    \n",
    "    Input:\n",
    "    - words (list): List of words from which stop words should be removed.\n",
    "    \n",
    "    Output:\n",
    "    - filtered_words (list): List of words with stop words removed.\n",
    "    \"\"\"\n",
    "    # Get the set of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words from the list\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words reduces the amount of data to be processed, which can improve the efficiency of text analysis. This step helps in focusing on the more meaningful words that contribute to the content's overall meaning. However, it's essential to choose stop words carefully, as some common words might still carry significant contextual information depending on the analysis goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Stemming is the process of reducing words to their base or root form, often by removing suffixes. For example, the words \"running\", \"runner\", and \"ran\" might all be reduced to the root \"run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem(words):\n",
    "    \"\"\"\n",
    "    Applies stemming to a list of words, reducing them to their root forms.\n",
    "\n",
    "    Args:\n",
    "    words (list): A list of words to be stemmed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of stemmed words.\n",
    "    \"\"\"\n",
    "    # Initialize the Porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    # Apply stemming to each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming helps in normalizing the text by converting different forms of a word to a common base form. This reduces the vocabulary size, which is beneficial for text analysis and retrieval. However, stemming can sometimes lead to over-reduction, where words with different meanings are reduced to the same stem, potentially causing confusion and loss of semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Lemmatization is the process of reducing words to their base or dictionary form, called lemmas, by considering the context in which they are used. Unlike stemming, lemmatization uses a vocabulary and morphological analysis to accurately convert words to their base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PMLS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_pos_tag):\n",
    "    if nltk_pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize(words):\n",
    "    \"\"\"\n",
    "    Applies lemmatization to a list of words, reducing them to their base forms.\n",
    "\n",
    "    Args:\n",
    "    words (list): A list of words to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lemmatized words.\n",
    "    \"\"\"\n",
    "    # Initialize the WordNet lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Get POS tags for the words\n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    # Apply lemmatization to each word with its POS tag\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    \n",
    "    return lemmatized_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization provides a more accurate reduction of words compared to stemming by considering the word's context and part of speech. This ensures that the words retain their true meaning after reduction. Lemmatization helps in improving the quality of the text analysis, making the resulting data more reliable and meaningful for subsequent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Applies a series of preprocessing steps to a tweet.\n",
    "    \n",
    "    Input:\n",
    "    - tweet (str): The tweet text to be preprocessed.\n",
    "    \n",
    "    Output:\n",
    "    - preprocessed (str): Preprocessed text.\n",
    "    \"\"\"\n",
    "    # Tokenization\n",
    "    words = tokenize(tweet)\n",
    "    \n",
    "    # Stop Word Removal\n",
    "    words_no_stopwords = remove_stopwords(words)\n",
    "    \n",
    "    # Stemming    \n",
    "    stemmed_words = stem(words_no_stopwords)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = lemmatize(words_no_stopwords)\n",
    "    \n",
    "    # Return after all preprocessing\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply preprocessing to the 'Tweet' column\n",
    "df['Preprocessed'] = df['Tweet'][:100].apply(preprocess_tweet)\n",
    "# print(df['Tweet'][:100].apply(preprocess_tweet))\n",
    "# print(lemmatize(['behaving']))\n",
    "\n",
    "# print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
